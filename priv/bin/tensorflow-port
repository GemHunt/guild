#!/usr/bin/python

# Copyright 2016 TensorHub, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Port style application providing TensorFlow support

This is not a true Erlang external port but is used in a similar way,
taking requests from stdin and responding over stdout. The protocol is
binary and described below.

A requests is:

  REQ = REF + CMD + ARG-COUNT + ARGS
  REF = uint (4)
  CMD = ushort (2)
  ARG-COUNT = uint (4)
  ARGS = ARG*
  ARG = ARG-LEN + ARG-VAL
  ARG-LEN = ulong (8)
  ARG_VAL = bytes (ARG-LEN)

A response is:

  RESP = REF + STATUS + PART-COUNT + PARTS
  REF = uint (4)
  STATUS = ushort (2)
  PART-COUNT = uint (4)
  PARTS = PART*
  PART = PART-LEN + PART-VAL
  PART-LEN = ulong (8)
  PART-VAL = bytes (PART-LEN)

"""
from __future__ import division

import glob
import imghdr
import json
import os
import struct
import sys
import time

STDIN = sys.stdin
STDOUT = sys.stdout
STDERR = sys.stderr

TRACE_STATS = False

# ===================================================================
# Protocol
# ===================================================================

class ErrorResponse(BaseException):

    def __init__(self, req, msg):
        self.req = req
        self.msg = msg

class ProtocolError(Exception):
    pass

class Request(object):

    def __init__(self, ref, cmd, args):
        self.ref = ref
        self.cmd = cmd
        self.args = args

def read_request():
    header_bytes = STDIN.read(10)
    if not header_bytes:
        return None
    ref, cmd, arg_count = struct.unpack(">IHI", header_bytes)
    args = []
    for _ in range(arg_count):
        arg_len, = struct.unpack(">Q", STDIN.read(8))
        args.append(STDIN.read(arg_len))
    return Request(ref, cmd, args)

def send_response(req, status, parts):
    STDOUT.write(struct.pack(">IHI", req.ref, status, len(parts)))
    for part in parts:
        STDOUT.write(struct.pack(">Q", len(part)))
        STDOUT.write(part)
    STDOUT.flush()

STATUS_OK = 0
STATUS_ERROR = 1

def send_ok_response(req, parts=[]):
    send_response(req, STATUS_OK, parts)

def send_error_response(req, parts):
    send_response(req, STATUS_ERROR, parts)

def not_found_response():
    return ["not_found"]

# ===================================================================
# Request cmd dispatch
# ===================================================================

CMD_TEST_PROTOCOL = 0
CMD_READ_IMAGE    = 1
CMD_LOAD_MODEL    = 2
CMD_RUN_MODEL     = 3
CMD_MODEL_INFO    = 4
CMD_MODEL_STATS   = 5

def handle_request(req):
    if req.cmd == CMD_READ_IMAGE:
        handle_read_image(req)
    elif req.cmd == CMD_LOAD_MODEL:
        handle_load_model(req)
    elif req.cmd == CMD_RUN_MODEL:
        handle_run_model(req)
    elif req.cmd == CMD_MODEL_INFO:
        handle_model_info(req)
    elif req.cmd == CMD_MODEL_STATS:
        handle_model_stats(req)
    elif req.cmd == CMD_TEST_PROTOCOL:
        handle_test_protocol(req)
    else:
        raise ProtocolError("cmd: %r" + req.cmd)

# ===================================================================
# Test protocol
# ===================================================================

def handle_test_protocol(req):
    send_ok_response(req, req.args)

# ===================================================================
# Read image
# ===================================================================

class Image(object):

    def __init__(self, event_file, tag, tf_image):
        self.event_file = event_file
        self.tag = tag
        self.height = tf_image.height
        self.width = tf_image.width
        self.colorspace = tf_image.colorspace
        self.image_bytes = tf_image.encoded_image_string

def handle_read_image(req):
    path, index = validate_read_image_args(req)
    cur = 0
    for image in iter_images(path):
        if cur == index:
            send_ok_response(req, image_response(image))
            break
        else:
            cur += 1
    else:
        raise ErrorResponse(req, "not found")

def validate_read_image_args(req):
    if len(req.args) != 2:
        raise ProtocolError("read-image requires 2 arguments")
    path = req.args[0]
    if not os.path.exists(path):
        raise ErrorResponse(req, "not found")
    index = int(req.args[1])
    if index < 0:
        raise ErrorResponse(req, "not found")
    return path, index

def iter_images(path):
    from tensorflow.python.summary.impl import event_file_loader
    for event_file in find_event_files(path):
        loader = event_file_loader.EventFileLoader(event_file)
        for event in loader.Load():
            if event.HasField("summary"):
                for value in event.summary.value:
                    if value.HasField("image"):
                        yield Image(event_file, value.tag, value.image)

def find_event_files(root_dir):
    pattern = "*.tfevents*"
    for path in glob.iglob(os.path.join(root_dir, pattern)):
        yield path
    for path in glob.iglob(os.path.join(root_dir, "**", pattern)):
        yield path

def image_response(image):
    return [
        image.event_file,
        image.tag,
        encode_image_dimensions(image.height, image.width, image.colorspace),
        encode_image_type(image.image_bytes),
        image.image_bytes
    ]

def encode_image_dimensions(h, w, d):
    return "%i %i %i" % (h, w, d)

def encode_image_type(image_bytes):
    guessed = imghdr.what(None, image_bytes)
    return guessed if guessed else ""

# ===================================================================
# Model support
# ===================================================================

def init_session(vars_path):
    import tensorflow as tf
    graph = tf.Graph()
    with graph.as_default():
        saver = tf.train.import_meta_graph(vars_path + ".meta", True)
    config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))
    session = tf.Session(config=config, graph=graph)
    saver.restore(session, vars_path)
    return session

class ModelError(Exception):
    pass

class ModelStats(object):

    N = 5 # observations used for weighted averages

    def __init__(self):
        self.reset()

    def reset(self):
        self._batches = []

    def start_batch(self, size):
        self._pending_start = self._timestamp()
        self._pending_batch_size = size

    def end_batch(self):
        batch_time = self._timestamp() - self._pending_start
        self._batches.append((self._pending_batch_size, batch_time))

    def _timestamp(_self):
        return int(time.time() * 1000000)

    def generate(self):
        return {
            "last_batch_time_ms": self._last_batch_time_ms(),
            "average_batch_time_ms": self._average_batch_time_ms(self.N),
            "predictions_per_second": self._predictions_per_second(self.N)
        }

    def _last_batch_time_ms(self):
        if self._batches:
            return self._batches[-1][1] / 1000
        else:
            return None

    def _average_batch_time_ms(self, n0):
        if self._batches:
            # Weighted average
            n = min(n0, len(self._batches))
            sample = self._batches[-n:]
            weighted_total = 0
            for i, val in enumerate(sample):
                weighted_total += (i + 1) * val[1] / 1000
            return weighted_total / (n * (n + 1) / 2)
        else:
            return None

    def _predictions_per_second(self, n0):
        if self._batches:
            # Weighted predictions
            n = min(n0, len(self._batches))
            sample = self._batches[-n:]
            weighted_predictions = 0
            weighted_seconds = 0
            for i, (predictions, time_us) in enumerate(sample):
                weighted_predictions += (i + 1) * predictions
                weighted_seconds += (i + 1) * (time_us / 1000000)
            return weighted_predictions / weighted_seconds
        else:
            return None

class Model(object):

    def __init__(self):
        self._serving_path = None
        self._session = None
        self._output_tensors = []
        self._output_keys = []
        self._input_tensor_map = {}
        self._stats = ModelStats()

    def ensure_serving_path(self, path):
        if self._serving_path != path:
            self._reset_for_path(path)

    def _reset_for_path(self, path):
        self._serving_path = None
        if self._session:
            self._session.close()
        self._stats.reset()
        session = init_session(path)
        self._session = session
        output_tensors, output_keys = self._graph_outputs(session.graph)
        self._output_tensors = output_tensors
        self._output_keys = output_keys
        self._input_tensor_map = self._graph_inputs(session.graph)
        self._serving_path = path

    def _graph_outputs(_self, graph):
        tensor_map = {}
        for val in graph.get_collection("outputs"):
            tensor_map.update(json.loads(val))
        keys = tensor_map.keys()
        tensors = [graph.get_tensor_by_name(tensor_map[key]) for key in keys]
        return tensors, keys

    def _graph_inputs(_self, graph):
        tensor_map = {}
        for val in graph.get_collection("inputs"):
            tensor_map.update(json.loads(val))
        return {
            key: graph.get_tensor_by_name(tensor_name)
            for key, tensor_name in tensor_map.items()
        }

    def run(self, instances):
        inputs = self._inputs_feed_dict(instances)
        self._stats.start_batch(len(instances))
        run_result = self._session_run(inputs)
        self._stats.end_batch()
        return self._run_result_outputs(run_result)

    def _session_run(self, inputs):
        if TRACE_STATS:
            return self._session_run_with_stats(inputs)
        else:
            return self._session.run(self._output_tensors, feed_dict=inputs)

    def _session_run_with_stats(self, inputs):
        # Working alternative to _session_run with tfprof stats
        import tensorflow as tf
        run_metadata = tf.RunMetadata()
        run_result = self._session.run(
            self._output_tensors,
            feed_dict=inputs,
            options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),
            run_metadata=run_metadata)
        tfprof = tf.contrib.tfprof
        _stats = tfprof.model_analyzer.print_model_analysis(
            self._session.graph,
            run_metadata=run_metadata,
            tfprof_options=tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY)
        STDERR.write(str(_stats) + "\n")
        # TODO: possibly integrate _stats here with self._stats
        return run_result

    def _inputs_feed_dict(self, instances):
        return {
            tensor: [inst[key] for inst in instances]
            for key, tensor in self._input_tensor_map.items()
        }

    def _run_result_outputs(self, run_result):
        items = [{}] * len(run_result[0]) if run_result else []
        for key_index, values in enumerate(run_result):
            key = self._output_keys[key_index]
            for item_index, value in enumerate(values):
                items[item_index][key] = value.tolist()
        return items

    def info(self):
        return {
            "path": self._serving_path,
            "inputs": self._inputs_info(),
            "outputs": self._outputs_info()
        }

    def _inputs_info(self):
        return {
            key: self._tensor_info(tensor)
            for key, tensor in self._input_tensor_map.items()
        }

    def _outputs_info(self):
        return {
            key: self._tensor_info(tensor)
            for key, tensor in zip(self._output_keys, self._output_tensors)
        }

    def _tensor_info(_self, tensor):
        return {
            "tensor": tensor.name,
            "dtype": tensor.dtype.name,
            "shape": str(tensor.get_shape())
        }

    def stats(self):
        return self._stats.generate()

active_model = Model()

def ensure_active_model_serving_path(path, req):
    try:
        active_model.ensure_serving_path(path)
    except ModelError, e:
        raise ErrorResponse(req, str(e))

# ===================================================================
# Load model
# ===================================================================

def handle_load_model(req):
    model_path = validate_model_path_args(req)
    ensure_active_model_serving_path(model_path, req)
    send_ok_response(req)

def validate_model_path_args(req):
    if len(req.args) != 1:
        raise ProtocolError("%s requires 1 argument" % req.name)
    model_path = req.args[0]
    if not os.path.exists(model_path):
        raise ErrorResponse(req, "not found")
    return model_path

# ===================================================================
# Run model
# ===================================================================

def handle_run_model(req):
    model_path, instances = validate_run_model_args(req)
    ensure_active_model_serving_path(model_path, req)
    try:
        outputs = active_model.run(instances)
    except ValueError, e:
        send_error_response(req, [str(e)])
    else:
        send_ok_response(req, [json.dumps(outputs)])

def validate_run_model_args(req):
    if len(req.args) != 2:
        raise ProtocolError("run-model requires 2 arguments")
    model_path = req.args[0]
    if not os.path.exists(model_path):
        raise ErrorResponse(req, "not found")
    try:
        instances = json.loads(req.args[1])
    except ValueError:
        raise ErrorResponse(req, "run request must be valid JSON")
    if type(instances) != list:
        raise ErrorResponse(req, "run request must be a list")
    return model_path, instances

# ===================================================================
# Model info
# ===================================================================

def handle_model_info(req):
    model_path = validate_model_path_args(req)
    ensure_active_model_serving_path(model_path, req)
    info = active_model.info()
    send_ok_response(req, [json.dumps(info)])

# ===================================================================
# Model stats
# ===================================================================

def handle_model_stats(req):
    model_path = validate_model_path_args(req)
    ensure_active_model_serving_path(model_path, req)
    stats = active_model.stats()
    send_ok_response(req, [json.dumps(stats)])

# ===================================================================
# Port loop
# ===================================================================

def port_loop():
    while True:
        req = read_request()
        if not req:
            break
        try:
            handle_request(req)
        except ErrorResponse, e:
            send_error_response(e.req, [e.msg])

# ===================================================================
# Run once (for debugging)
# ===================================================================

def run_once(model_path, single_input_path):
    active_model.ensure_serving_path(model_path)
    inputs = [json.load(open(single_input_path, "r"))]
    outputs = active_model.run(inputs)
    print json.dumps(outputs)

# ===================================================================
# Test
# ===================================================================

def test():
    test_model_stats()
    print "All tests passed"

def test_model_stats():
    def generate_stats(batches):
        stats = ModelStats()
        stats._batches = batches
        return stats.generate()

    def assert_equals(stats, name, expected):
        actual = stats[name]
        if actual != expected:
            STDERR.write("ERROR: %s was %s, expected %s\n"
                         % (name, actual, expected))
            sys.exit(1)

    s1 = generate_stats([])
    assert_equals(s1, "last_batch_time_ms", None)
    assert_equals(s1, "average_batch_time_ms", None)
    assert_equals(s1, "predictions_per_second", None)

    s2 = generate_stats([(1, 10000)])
    assert_equals(s2, "last_batch_time_ms", 10.0)
    assert_equals(s2, "average_batch_time_ms", 10.0)
    assert_equals(s2, "predictions_per_second", 100.0)

    s3 = generate_stats([(1, 10000), (1, 10000)])
    assert_equals(s3, "last_batch_time_ms", 10.0)
    assert_equals(s3, "average_batch_time_ms", 10.0)
    assert_equals(s3, "predictions_per_second", 100.0)

    s4 = generate_stats([(1, 10000), (1, 20000)])
    assert_equals(s4, "last_batch_time_ms", 20.0)
    assert_equals(s4, "average_batch_time_ms", 50000 / 3 / 1000)
    assert_equals(s4, "predictions_per_second", 1 / (50000 / 3 / 1000000))

    s5 = generate_stats(
        [(1, 60000),
         (1, 50000),
         (1, 40000),
         (1, 30000),
         (1, 20000),
         (1, 10000)])
    assert_equals(s5, "last_batch_time_ms", 10.0)
    assert_equals(s5, "average_batch_time_ms", 350000 / 15 / 1000)
    assert_equals(s5, "predictions_per_second", 1 / (350000 / 15 / 1000000))

# ===================================================================
# Main
# ===================================================================

def script_error(msg):
    STDERR.write(msg + "\n")
    sys.exit(1)

if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("cmd", metavar="CMD", nargs="?",
                   help="optional command: run | test")
    p.add_argument("--model", metavar="PATH",
                   help="model path")
    p.add_argument("--single-input",  metavar="PATH",
                   help=("single input path; expects a single instance "
                         "that will run in a list"))
    args = p.parse_args()
    if args.cmd == "run":
        if not args.model:
            script_error("--model required for run command")
        if not args.single_input:
            script_error("--inputs required for run command")
        run_once(args.model, args.single_input)
    elif args.cmd == "test":
        test()
    else:
        port_loop()
