#!/usr/bin/python

# Copyright 2016 TensorHub, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Port style application providing TensorFlow support

This is not a true Erlang external port but is used in a similar way,
taking requests from stdin and responding over stdout. The protocol is
text line based using single LF (ASCII 10) chars as line delimiters.

A request consists of a single tab (ASCII 9) separated list of
parts. A request must contain two parts and may contain more.

Separators:

  TAB:            ASCII 9
  LF:             ASCII 10

Request:

  REQUEST:        REFERENCE + TAB + NAME [ + TAB + ARG ]* + LF

  REFERENCE:      A value that is unique to the request

  NAME:           The name of the request

  ARG:            A string representation of a request argument

A response consists of a single response line followed by zero or more
response parts, each terminated by a line. The response itself is
terminated by a final line.

The response line is the request reference and response status
separated by a tab.

Response:

  RESPONSE:       RESPONSE LINE + LF [ + PART + LF ]* + LF

  RESPONSE LINE:  REFERENCE + TAB + STATUS

  STATUS:         'ok' | 'error'

  PART:           A string representation of a response part

Request arguments and response parts are specific to a request type,
which is identified by its name.

Arguments must not contain tabs or line endings. Parts must not
contain line endings. In general these values should be encoded to
avoid either of these separators.

"""
import base64
import glob
import imghdr
import json
import os
import sys

STDIN = sys.stdin
STDOUT = sys.stdout
STDERR = sys.stderr

# ===================================================================
# Protocol
# ===================================================================

class ErrorResponse(BaseException):

    def __init__(self, req, msg):
        self.req = req
        self.msg = msg

class ProtocolError(Exception):
    pass

class Request(object):

    def __init__(self, line):
        parts = line.split("\t")
        if len(parts) < 2:
            raise ProtocolError("request line " + repr(line))
        self.ref = parts[0]
        self.name = parts[1]
        self.args = parts[2:]

def read_request():
    line = STDIN.readline()[:-1]
    return Request(line) if line else None

def send_response(req, status, parts):
    STDOUT.write(req.ref)
    STDOUT.write("\t")
    STDOUT.write(status)
    STDOUT.write("\n")
    for part in parts:
        STDOUT.write(part)
        STDOUT.write("\n")
    STDOUT.write("\n")
    STDOUT.flush()

def send_ok_response(req, parts=[]):
    send_response(req, "ok", parts)

def send_error_response(req, parts):
    send_response(req, "error", parts)

def not_found_response():
    return ["not_found"]

# ===================================================================
# Request dispatch
# ===================================================================

def handle_request(req):
    if req.name == "read-image":
        handle_read_image(req)
    elif req.name == "load-model":
        handle_load_model(req)
    elif req.name == "run-model":
        handle_run_model(req)
    elif req.name == "model-info":
        handle_model_info(req)
    else:
        raise ProtocolError("unknown " + repr(req.name))

# ===================================================================
# Read image
# ===================================================================

class Image(object):

    def __init__(self, event_file, tag, tf_image):
        self.event_file = event_file
        self.tag = tag
        self.height = tf_image.height
        self.width = tf_image.width
        self.colorspace = tf_image.colorspace
        self.image_bytes = tf_image.encoded_image_string

def handle_read_image(req):
    path, index = validate_read_image_args(req)
    cur = 0
    for image in iter_images(path):
        if cur == index:
            send_response(req, "ok", image_response(image))
            break
        else:
            cur += 1
    else:
        raise ErrorResponse(req, "not found")

def validate_read_image_args(req):
    if len(req.args) != 2:
        raise ProtocolError("read-image requires 2 arguments")
    path = req.args[0]
    if not os.path.exists(path):
        raise ErrorResponse(req, "not found")
    index = int(req.args[1])
    if index < 0:
        raise ErrorResponse(req, "not found")
    return path, index

def iter_images(path):
    from tensorflow.python.summary.impl import event_file_loader
    for event_file in find_event_files(path):
        loader = event_file_loader.EventFileLoader(event_file)
        for event in loader.Load():
            if event.HasField("summary"):
                for value in event.summary.value:
                    if value.HasField("image"):
                        yield Image(event_file, value.tag, value.image)

def find_event_files(root_dir):
    pattern = "*.tfevents*"
    for path in glob.iglob(os.path.join(root_dir, pattern)):
        yield path
    for path in glob.iglob(os.path.join(root_dir, "**", pattern)):
        yield path

def image_response(image):
    return [
        image.event_file,
        image.tag,
        encode_image_dimensions(image.height, image.width, image.colorspace),
        encode_image_type(image.image_bytes),
        encode_image_bytes(image.image_bytes)
    ]

def encode_image_dimensions(h, w, d):
    return "%i %i %i" % (h, w, d)

def encode_image_type(image_bytes):
    guessed = imghdr.what(None, image_bytes)
    return guessed if guessed else ""

def encode_image_bytes(image_bytes):
    return base64.b64encode(image_bytes)

# ===================================================================
# Model support
# ===================================================================

def init_session(vars_path):
    import tensorflow as tf
    graph = tf.Graph()
    with graph.as_default():
        saver = tf.train.import_meta_graph(vars_path + ".meta", True)
    config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))
    session = tf.Session(config=config, graph=graph)
    saver.restore(session, vars_path)
    return session

class ModelError(Exception):
    pass

class Model(object):

    _serving_path = None
    _session = None
    _output_tensors = []
    _output_keys = []
    _input_tensor_map = {}

    def ensure_serving_path(self, path):
        if self._serving_path != path:
            self._reset_for_path(path)

    def _reset_for_path(self, path):
        self._serving_path = None
        if self._session:
            self._session.close()
        session = init_session(path)
        self._session = session
        output_tensors, output_keys = self._graph_outputs(session.graph)
        self._output_tensors = output_tensors
        self._output_keys = output_keys
        self._input_tensor_map = self._graph_inputs(session.graph)
        self._serving_path = path

    def _graph_outputs(_self, graph):
        tensor_map = {}
        for val in graph.get_collection("outputs"):
            tensor_map.update(json.loads(val))
        if not tensor_map:
            raise ModelError("no outputs")
        keys = tensor_map.keys()
        tensors = [graph.get_tensor_by_name(tensor_map[key]) for key in keys]
        return tensors, keys

    def _graph_inputs(_self, graph):
        tensor_map = {}
        for val in graph.get_collection("inputs"):
            tensor_map.update(json.loads(val))
        return {
            key: graph.get_tensor_by_name(tensor_name)
            for key, tensor_name in tensor_map.items()
        }

    def run(self, instances):
        inputs = self._inputs_feed_dict(instances)
        run_result = self._session.run(self._output_tensors, feed_dict=inputs)
        return self._run_result_outputs(run_result)

    def _inputs_feed_dict(self, instances):
        return {
            tensor: [inst[key] for inst in instances]
            for key, tensor in self._input_tensor_map.items()
        }

    def _run_result_outputs(self, run_result):
        items = [{}] * len(run_result[0]) if run_result else []
        for key_index, values in enumerate(run_result):
            key = self._output_keys[key_index]
            for item_index, value in enumerate(values):
                items[item_index][key] = value.tolist()
        return items

    def info(self):
        return {
            "path": self._serving_path,
            "inputs": self._inputs_info(),
            "outputs": self._outputs_info()
        }

    def _inputs_info(self):
        return {
            key: self._tensor_info(tensor)
            for key, tensor in self._input_tensor_map.items()
        }

    def _outputs_info(self):
        return {
            key: self._tensor_info(tensor)
            for key, tensor in zip(self._output_keys, self._output_tensors)
        }

    def _tensor_info(_self, tensor):
        return {
            "tensor": tensor.name,
            "dtype": tensor.dtype.name,
            "shape": str(tensor.get_shape())
        }

active_model = Model()

def ensure_active_model_serving_path(path, req):
    try:
        active_model.ensure_serving_path(path)
    except ModelError, e:
        raise ErrorResponse(req, str(e))

# ===================================================================
# Load model
# ===================================================================

def handle_load_model(req):
    model_path = validate_load_model_args(req)
    ensure_active_model_serving_path(model_path, req)
    send_ok_response(req)

def validate_load_model_args(req):
    if len(req.args) != 1:
        raise ProtocolError("load-model requires 1 argument")
    model_path = req.args[0]
    if not os.path.exists(model_path):
        raise ErrorResponse(req, "not found")
    return model_path

# ===================================================================
# Run model
# ===================================================================

def handle_run_model(req):
    model_path, instances = validate_run_model_args(req)
    ensure_active_model_serving_path(model_path)
    try:
        outputs = active_model.run(instances)
    except ValueError, e:
        send_error_response(req, [str(e)])
    else:
        send_ok_response(req, [json.dumps(outputs)])

def validate_run_model_args(req):
    if len(req.args) != 2:
        raise ProtocolError("run-model requires 2 arguments")
    model_path = req.args[0]
    if not os.path.exists(model_path):
        raise ErrorResponse(req, "not found")
    try:
        instances = json.loads(req.args[1])
    except ValueError:
        raise ErrorResponse(req, "run request must be valid JSON")
    if type(instances) != list:
        raise ErrorResponse(req, "run request must be a list")
    return model_path, instances

# ===================================================================
# Model info
# ===================================================================

def handle_model_info(req):
    model_path = validate_model_info_args(req)
    ensure_active_model_serving_path(model_path, req)
    info = active_model.info()
    send_ok_response(req, [json.dumps(info)])

def validate_model_info_args(req):
    if len(req.args) != 1:
        raise ProtocolError("load-model requires 1 argument")
    model_path = req.args[0]
    if not os.path.exists(model_path):
        raise ErrorResponse(req, "not found")
    return model_path

# ===================================================================
# Main loop
# ===================================================================

if __name__ == "__main__":
    while True:
        req = read_request()
        if not req:
            break
        try:
            handle_request(req)
        except ErrorResponse, e:
            send_error_response(e.req, [e.msg])
